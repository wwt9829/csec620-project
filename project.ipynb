{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project\n",
    "*Team #1 - Connie Dunlap, Ryne Krueger, Jackson Robbins, Wyatt Tauber*\n",
    "\n",
    "*CSEC-520/620 Cyber Analytics & Machine Learning*\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "This notebook requires the following packages:\n",
    "* TBD\n",
    "\n",
    "The following files and folder should be in the same directory as this notebook:\n",
    "* TBD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Data Selection & Processing\n",
    "\n",
    "Process the data from raw executable files into useable features for our selected ML models. See the project PDF for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1.2 Feature Extraction\n",
    "Author: Wyatt Tauber\n",
    "\n",
    "Convert the raw executable files into sha256_hash,packer,entropy lists\n",
    "\"\"\"\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "# the feature lists\n",
    "packed_data = []\n",
    "unpacked_data = []\n",
    "\n",
    "# loop through each packer directory and process the files\n",
    "for dir in os.listdir():\n",
    "    if '.' not in dir:\n",
    "        for file in os.listdir(dir):\n",
    "            # call radare2 to calcualte entropy and store the result\n",
    "            command = \"rahash2 -a entropy\"\n",
    "            string = dir + \"/\" + file\n",
    "            proc = subprocess.Popen([command + \" \" + string], stdout=subprocess.PIPE, shell=True)\n",
    "            (out, err) = proc.communicate()\n",
    "\n",
    "            # get the hash and entropy of the file\n",
    "            file_info = out.decode().strip().split()\n",
    "            \n",
    "            file_hash = file_info[0].split(\"/\")[-1][0:-1]\n",
    "            file_entropy = file_info[-1]\n",
    "            \n",
    "            # place the file information in the appropriate list\n",
    "            if 'unpacked' in file_hash:\n",
    "                file_hash = file_hash.split('_')[-1]\n",
    "                file_packer = \"none\"\n",
    "                datapoint = [file_hash,file_packer,file_entropy]\n",
    "                unpacked_data.append(datapoint)\n",
    "            else:\n",
    "                file_packer = dir\n",
    "                datapoint = [file_hash,file_packer,file_entropy]\n",
    "                packed_data.append(datapoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1.3 Dataset Cleaning\n",
    "Author: Jackson Robbins, Wyatt Tauber\n",
    "\n",
    "Convert the lists into numeric values supported by Numpy\n",
    "\"\"\"\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# create train/test splits of all of the data\n",
    "total_data = packed_data + unpacked_data\n",
    "train_set, test_set = train_test_split(total_data, test_size=0.4, random_state=7294802)\n",
    "\n",
    "# create the label and data lists for the splits\n",
    "train_label = []\n",
    "test_label = []\n",
    "train_data = []\n",
    "test_data = []\n",
    "\n",
    "for data in train_set:\n",
    "    #  store the training labels\n",
    "    if data[1] == \"none\":\n",
    "        train_label.append(0)\n",
    "    else:\n",
    "        train_label.append(1)\n",
    "        \n",
    "    # store the training data\n",
    "    train_data.append(float(data[2]))\n",
    "   \n",
    "for data in test_set:\n",
    "    # store the testing labels \n",
    "    if data[1] == \"none\":\n",
    "        test_label.append(0)\n",
    "    else:\n",
    "        test_label.append(1)\n",
    "        \n",
    "    # store the testing data\n",
    "    test_data.append(float(data[2]))\n",
    "    \n",
    "# convert the data to Numpy arrays\n",
    "train_label = np.asarray(train_label)\n",
    "test_label = np.asarray(test_label)\n",
    "train_data = np.asarray(train_data)\n",
    "test_data = np.asarray(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Experiments & Results\n",
    "\n",
    "Implement various ML models, tune hyperparameters, and present metrics. See the project PDF for details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9289940828402367\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "2.1 Logistic Regression\n",
    "Author: Jackson Robbins, Wyatt Tauber\n",
    "\n",
    "An sklearn logistic regression implementation\n",
    "\"\"\"\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# reshape the data into 2 dimensions\n",
    "logistic_train_data = train_data.reshape(-1,1)\n",
    "logistic_test_data = test_data.reshape(-1,1)\n",
    "\n",
    "\n",
    "# perform logistic regression on the train data\n",
    "# TODO: \"hyperparameter\" tuning? plots?\n",
    "logisticRegr = LogisticRegression()\n",
    "logisticRegr.fit(logistic_train_data, train_label)\n",
    "\n",
    "# make predictions on the test data\n",
    "score = logisticRegr.score(logistic_test_data, test_label)\n",
    "\n",
    "# print the metrics\n",
    "# TODO: more metrics? plots?\n",
    "print(\"accuracy:\",score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9053254437869822\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "2.2 Linear SVM\n",
    "Author: Jackson Robbins, Wyatt Tauber\n",
    "\n",
    "A sklearn linear SVM implementation\n",
    "\"\"\"\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# reshape the data into 2 dimensions\n",
    "svm_train_data = train_data.reshape(-1,1)\n",
    "svm_test_data = test_data.reshape(-1,1)\n",
    "\n",
    "# run linear SVM on the train data\n",
    "clf = SVC(kernel='linear')\n",
    "clf.fit(svm_train_data, train_label)\n",
    "\n",
    "# make predictions on the test data\n",
    "# TODO: hyperparameter tuning? plots?\n",
    "svm_score = clf.score(svm_test_data,test_label)\n",
    "\n",
    "# print the metrics\n",
    "# TODO: more metrics? plots?\n",
    "print(\"accuracy:\",svm_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 k-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "2.3 k-Means\n",
    "Author: Jackson Robbins, Wyatt Tauber\n",
    "\n",
    "A sklearn k-Means implementation\n",
    "# TODO: finish!\n",
    "\"\"\"\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "km_train_data = train_data.reshape(-1,1)\n",
    "km_test_data = test_data.reshape(-1,1)\n",
    "\n",
    "km = KMeans(\n",
    "    n_clusters=2, init='random',\n",
    "    n_init=10, max_iter=300, \n",
    "    tol=1e-04\n",
    ")\n",
    "\n",
    "y_km = km.fit_predict(km_train_data)#, train_label)\n",
    "#kmeans_score = km.score(km_test_data, test_label)\n",
    "print(y_km)\n",
    "#print(km_train_data)\n",
    "\n",
    "\"\"\"\n",
    "plt.scatter(\n",
    "    km_train_data[y_km == 0, 0], km_train_data[y_km == 0, 1],\n",
    "    s=50, c='lightgreen',\n",
    "    marker='s', edgecolor='black',\n",
    "    label='cluster 1'\n",
    ")\n",
    "\n",
    "\n",
    "plt.scatter(\n",
    "    km_train_data[y_km == 1, 0], km_train_data[y_km == 1, 1],\n",
    "    s=50, c='orange',\n",
    "    marker='o', edgecolor='black',\n",
    "    label='cluster 2'\n",
    ")\n",
    "\n",
    "plt.legend(scatterpoints=1)\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\"\"\"\n",
    "\n",
    "plt.figure()\n",
    "#a = [1,2,5,6,9,11,15,17,18]\n",
    "plt.hlines(1,0.2,8)  # Draw a horizontal line\n",
    "plt.eventplot(train_data, orientation='horizontal', colors='b')\n",
    "plt.show()\n",
    "\n",
    "#print(\"accuracy:\",kmeans_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}